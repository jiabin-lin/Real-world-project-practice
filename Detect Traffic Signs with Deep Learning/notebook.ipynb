{"cells":[{"source":"![Traffic lights over urban intersection.](traffic.jpg)","metadata":{},"id":"9bef1e92-37c0-4f79-a27a-74f017644cdf","cell_type":"markdown"},{"source":"**Challenges in Traffic Sign Management**\n\nTraffic signs convey vital information to drivers. However, their effectiveness can be hindered by various factors, such as their visibility under different lighting conditions or the presence of obstructions.\n\n\n**Training Traffic Sign Detection Models**\n\nTo address these challenges and enhance stop sign and traffic light detection capabilities, advanced technologies such as deep learning and computer vision have gained significant attention. In this project, you'll train an object detection model on 6 images of stop signs and 6 images of traffic lights, taken from various angles and lighting conditions. This training teaches the model to classify and locate these signs in images, improving its robustness and reliability for real-world applications.","metadata":{},"id":"1c2bc1d6-3780-4d6f-8016-c90898817887","cell_type":"markdown"},{"source":"# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# Load preprocessed images and the corresponding labels\nimage, labels = np.load('batch.npy',allow_pickle=True).tolist()\n\n# hyperparameters\ninput_size = image.shape[1] # dimension of input image\nnum_classes = labels['classifier_head'].shape[1] # number of classes\nDROPOUT_FACTOR = 0.2 # dropout probability\n\n# visualize one example preprocessed image\nplt.imshow(image[2])\nplt.axis(\"off\")","metadata":{"executionCancelledAt":null,"executionTime":116,"lastExecutedAt":1719822409776,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# Load preprocessed images and the corresponding labels\nimage, labels = np.load('batch.npy',allow_pickle=True).tolist()\n\n# hyperparameters\ninput_size = image.shape[1] # dimension of input image\nnum_classes = labels['classifier_head'].shape[1] # number of classes\nDROPOUT_FACTOR = 0.2 # dropout probability\n\n# visualize one example preprocessed image\nplt.imshow(image[2])\nplt.axis(\"off\")","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":185,"type":"stream"}},"lastExecutedByKernel":"ae3bcaaf-0969-4666-ba98-62f56f2a3ea7"},"id":"2f26fc58-2bcd-45a6-841f-cbcdd391e2f2","cell_type":"code","execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":"(-0.5, 223.5, 223.5, -0.5)"},"metadata":{},"execution_count":31},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{"image/png":{"width":389,"height":389}}}]},{"source":"# Define the object detection model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\ndef create_model(input_size, num_classes, dropout_factor):\n    inputs = Input(shape=(input_size, input_size, 3))\n    \n    # Convolutional layers\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(dropout_factor)(x)\n    \n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(dropout_factor)(x)\n    \n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(dropout_factor)(x)\n    \n    # Flatten and fully connected layers\n    x = Flatten()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(dropout_factor)(x)\n    \n    # Output layer\n    outputs = Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs, outputs)\n    return model\n\nmodel = create_model(input_size, num_classes, DROPOUT_FACTOR)\nmodel.summary()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"lastExecutedByKernel":null,"outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":101,"type":"stream"}}},"id":"84cdd85c-afb2-4f4f-b66c-8b158eeac8d2","cell_type":"code","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n conv2d_9 (Conv2D)           (None, 224, 224, 32)      896       \n                                                                 \n max_pooling2d_9 (MaxPooling  (None, 112, 112, 32)     0         \n 2D)                                                             \n                                                                 \n dropout_12 (Dropout)        (None, 112, 112, 32)      0         \n                                                                 \n conv2d_10 (Conv2D)          (None, 112, 112, 64)      18496     \n                                                                 \n max_pooling2d_10 (MaxPoolin  (None, 56, 56, 64)       0         \n g2D)                                                            \n                                                                 \n dropout_13 (Dropout)        (None, 56, 56, 64)        0         \n                                                                 \n conv2d_11 (Conv2D)          (None, 56, 56, 128)       73856     \n                                                                 \n max_pooling2d_11 (MaxPoolin  (None, 28, 28, 128)      0         \n g2D)                                                            \n                                                                 \n dropout_14 (Dropout)        (None, 28, 28, 128)       0         \n                                                                 \n flatten_3 (Flatten)         (None, 100352)            0         \n                                                                 \n dense_20 (Dense)            (None, 256)               25690368  \n                                                                 \n dropout_15 (Dropout)        (None, 256)               0         \n                                                                 \n dense_21 (Dense)            (None, 2)                 514       \n                                                                 \n=================================================================\nTotal params: 25,784,130\nTrainable params: 25,784,130\nNon-trainable params: 0\n_________________________________________________________________\n"}]},{"source":"# Import necessary libraries\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\n\n# Assuming X and y are your data and labels\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(image, labels['classifier_head'], test_size=0.2, random_state=42)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model for 20 epochs\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n\n# Calculate training accuracy\ntrain_accuracy = history.history['accuracy'][-1]\naccuracy = train_accuracy","metadata":{"executionCancelledAt":null,"executionTime":5894,"lastExecutedAt":1719822415884,"lastExecutedByKernel":"ae3bcaaf-0969-4666-ba98-62f56f2a3ea7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\n\n# Assuming X and y are your data and labels\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(image, labels['classifier_head'], test_size=0.2, random_state=42)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model for 20 epochs\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n\n# Calculate training accuracy\ntrain_accuracy = history.history['accuracy'][-1]\naccuracy = train_accuracy","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"da2b87e8-a9f4-4318-916f-9f785d1eadc6","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1/20\n1/1 [==============================] - 1s 838ms/step - loss: 12.2130 - accuracy: 0.6667 - val_loss: 1882.6698 - val_accuracy: 0.6667\nEpoch 2/20\n1/1 [==============================] - 0s 297ms/step - loss: 3998.1902 - accuracy: 0.4444 - val_loss: 380.4416 - val_accuracy: 0.6667\nEpoch 3/20\n1/1 [==============================] - 0s 297ms/step - loss: 822.7932 - accuracy: 0.4444 - val_loss: 302.5822 - val_accuracy: 0.3333\nEpoch 4/20\n1/1 [==============================] - 0s 271ms/step - loss: 310.6441 - accuracy: 0.5556 - val_loss: 228.1290 - val_accuracy: 0.3333\nEpoch 5/20\n1/1 [==============================] - 0s 289ms/step - loss: 311.6324 - accuracy: 0.5556 - val_loss: 113.9362 - val_accuracy: 0.3333\nEpoch 6/20\n1/1 [==============================] - 0s 290ms/step - loss: 146.1648 - accuracy: 0.5556 - val_loss: 43.3720 - val_accuracy: 0.3333\nEpoch 7/20\n1/1 [==============================] - 0s 290ms/step - loss: 66.2005 - accuracy: 0.5556 - val_loss: 9.8491 - val_accuracy: 0.3333\nEpoch 8/20\n1/1 [==============================] - 0s 291ms/step - loss: 22.0026 - accuracy: 0.4444 - val_loss: 1.0123 - val_accuracy: 0.3333\nEpoch 9/20\n1/1 [==============================] - 0s 281ms/step - loss: 0.0399 - accuracy: 1.0000 - val_loss: 2.8645 - val_accuracy: 0.6667\nEpoch 10/20\n1/1 [==============================] - 0s 282ms/step - loss: 2.9114 - accuracy: 0.6667 - val_loss: 2.0681 - val_accuracy: 0.6667\nEpoch 11/20\n1/1 [==============================] - 0s 264ms/step - loss: 1.2428 - accuracy: 0.6667 - val_loss: 1.3279 - val_accuracy: 0.6667\nEpoch 12/20\n1/1 [==============================] - 0s 290ms/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 0.8333 - val_accuracy: 0.3333\nEpoch 13/20\n1/1 [==============================] - 0s 292ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 0.6667\nEpoch 14/20\n1/1 [==============================] - 0s 273ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.8013 - val_accuracy: 0.3333\nEpoch 15/20\n1/1 [==============================] - 0s 297ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.2866 - val_accuracy: 0.3333\nEpoch 16/20\n1/1 [==============================] - 0s 300ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 1.7446 - val_accuracy: 0.3333\nEpoch 17/20\n1/1 [==============================] - 0s 304ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 2.0544 - val_accuracy: 0.3333\nEpoch 18/20\n1/1 [==============================] - 0s 359ms/step - loss: 0.1907 - accuracy: 0.8889 - val_loss: 1.7783 - val_accuracy: 0.3333\n"}],"execution_count":33}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}